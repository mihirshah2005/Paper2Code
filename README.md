# ğŸ“ PaperCoder: Smart Research Assistant

PaperCoder is an **AI-driven research automation system** designed to assist researchers in planning, coding, and evaluating machine learning experiments. This project integrates **Large Language Models (LLMs)** and iterative feedback loops to refine code generation pipelines and enhance downstream system performance.  

This work was part of a research project at the **National University of Singapore** exploring how feedback loops and prompt engineering can optimize AI-driven research assistants.  

---

## ğŸš€ Features
### ğŸ”¹ 1. Stage-aware Critique Analysis
Classifies system critiques into three stages:  
- ğŸ§  **Planning**: Checks for missing or incorrect high-level research designs (e.g., wrong algorithm selection).  
- ğŸ§ª **Analyzing**: Detects issues in initial code generation, data processing logic, and module implementation.  
- ğŸ“Š **Evaluating**: Identifies faults in evaluation metrics, test cases, and reporting.

âœ… **Why it matters:**  
This allows PaperCoder to **pinpoint failure sources** and optimize stage-specific processes for better overall code quality.  

---

### ğŸ” 2. Feedback Loop Integration
Introduces an **iterative refinement mechanism** in the analyzing phase:  
1. Collects critiques from the first code generation pass.  
2. Feeds them back into the LLM with tailored prompts.  
3. Generates improved code for downstream evaluation.  

âœ… **Why it matters:**  
Mimics human researchers refining their code after peer review, enabling PaperCoder to "learn" from its mistakes within one generation cycle.  

---

### ğŸ¤– 3. LLM Prompt Engineering
Customizes prompts sent to the LLM for each stage:  
- In **Analyzing**, prompts request modular, maintainable code with explicit comments and alignment to paper methodologies.  
- Added **severity-level tagging** (High, Medium, Low) for critiques to prioritize critical fixes.

âœ… **Why it matters:**  
Improves first-try correctness and reduces cascading errors into later stages.  

---

### ğŸ“Š 4. Evaluation Pipeline with Dynamic Stage-Level Fault Analysis
Benchmarks generated code against gold-standard repositories:  
- Compares implementation details.  
- Tracks severity levels of critiques.  
- Outputs structured JSON and LaTeX reports summarizing results.  

âœ… **Why it matters:**  
Provides an **objective measure** of code fidelity and highlights areas for improvement.  

---

## ğŸ† Achievements
- Integrated and deployed as part of a research project on **AI-driven research automation with iterative refinement methods**.  
- Currently being drafted into a research paper for publication.  

---

## ğŸ›  Tech Stack
- **Python**: Core backend logic  
- **OpenAI API**: LLM-based code generation  
- **tqdm, argparse**: CLI tooling and progress tracking  
- **AWS S3**: (Optional) Artifact storage and retrieval  

---

## ğŸ“‚ Project Structure
PaperCoder/
â”œâ”€â”€ analyzing/
â”‚ â”œâ”€â”€ analyzing.py
â”‚ â”œâ”€â”€ feedback_loop.py
â”œâ”€â”€ utils/
â”‚ â”œâ”€â”€ extract_planning.py
â”‚ â”œâ”€â”€ evaluation.py
â”œâ”€â”€ outputs/
â”‚ â””â”€â”€ generated_code/
â”œâ”€â”€ README.md


---

## ğŸ“œ License
[MIT License](LICENSE)

---

## ğŸ‘¤ Author
**Mihir Shah**  
ğŸ“§ mihirsunilshah@gmail.com  
ğŸŒ [LinkedIn](https://linkedin.com/in/mihirsunilshah) â€¢ [GitHub](https://github.com/mihirshah2005)
